
@article{berry1995,
  title = {Using Linear Algebra for Intelligent Information Retrieval},
  author = {Berry, Michael W. and Dumais, Susan T. and O'Brien, Gavin W.},
  year = {1995},
  volume = {37},
  pages = {573--595},
  publisher = {{SIAM}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\RR58X3XG\\Berry et al. - 1995 - Using linear algebra for intelligent information r.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\K4VVY48V\\1037127.html},
  journal = {SIAM review},
  keywords = {Embeddings,NLP},
  number = {4}
}

@article{blei2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  volume = {3},
  pages = {993--1022},
  publisher = {{JMLR. org}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\PD9SIEF8\\Blei et al. - 2003 - Latent dirichlet allocation.pdf},
  journal = {the Journal of machine Learning research},
  keywords = {Embeddings,NLP}
}

@article{conneau2018,
  title = {Senteval: {{An}} Evaluation Toolkit for Universal Sentence Representations},
  shorttitle = {Senteval},
  author = {Conneau, Alexis and Kiela, Douwe},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1803.05449},
  eprinttype = {arxiv},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\68VAVMDP\\Conneau and Kiela - 2018 - Senteval An evaluation toolkit for universal sent.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\QISHR4LV\\1803.html},
  journal = {arXiv preprint arXiv:1803.05449},
  keywords = {Embeddings}
}

@article{conneau2019,
  title = {Unsupervised Cross-Lingual Representation Learning at Scale},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  archiveprefix = {arXiv},
  eprint = {1911.02116},
  eprinttype = {arxiv},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\FD93ZRR6\\Conneau et al. - 2019 - Unsupervised cross-lingual representation learning.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\GJ3JEUBN\\1911.html},
  journal = {arXiv preprint arXiv:1911.02116},
  keywords = {BERT,Embeddings,NLP}
}

@article{deerwester1990,
  title = {Indexing by Latent Semantic Analysis},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  year = {1990},
  volume = {41},
  pages = {391--407},
  publisher = {{Wiley Online Library}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\YSPY2TKF\\Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\3QVT4UJ4\\(SICI)1097-4571(199009)416391AID-ASI13.0.html},
  journal = {Journal of the American society for information science},
  keywords = {Embeddings,NLP},
  number = {6}
}

@inproceedings{dumais1988,
  title = {Using Latent Semantic Analysis to Improve Access to Textual Information},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems},
  author = {Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Deerwester, Scott and Harshman, Richard},
  year = {1988},
  pages = {281--285},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\8XIGQ3EM\\Dumais et al. - 1988 - Using latent semantic analysis to improve access t.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\VX9F8AVP\\57167.html},
  keywords = {Embeddings,NLP}
}

@article{harris1954,
  title = {Distributional Structure},
  author = {Harris, Zellig S.},
  year = {1954},
  volume = {10},
  pages = {146--162},
  publisher = {{Taylor \& Francis}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\99EYM5II\\Harris - 1954 - Distributional structure.pdf},
  journal = {Word},
  keywords = {Embeddings,NLP},
  number = {2-3}
}

@article{henriques2012,
  title = {Exploratory Geospatial Data Analysis Using the {{GeoSOM}} Suite},
  author = {Henriques, Roberto and Bacao, Fernando and Lobo, Victor},
  year = {2012},
  volume = {36},
  pages = {218--232},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\6P28EDC6\\S0198971511001141.html},
  journal = {Computers, Environment and Urban Systems},
  keywords = {SOM},
  number = {3}
}

@inproceedings{hofmann1999,
  title = {Probabilistic Latent Semantic Indexing},
  booktitle = {Proceedings of the 22nd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Hofmann, Thomas},
  year = {1999},
  pages = {50--57},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\YY2CTWDC\\Hofmann - 1999 - Probabilistic latent semantic indexing.pdf},
  keywords = {Embeddings,NLP}
}

@inproceedings{hu2008,
  title = {Collaborative Filtering for Implicit Feedback Datasets},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
  year = {2008},
  pages = {263--272},
  publisher = {{Ieee}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\2MSUVD8I\\4781121.html},
  keywords = {Recommender}
}

@article{ji2019,
  title = {Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection},
  shorttitle = {Visual Exploration of Neural Document Embedding in Information Retrieval},
  author = {Ji, Xiaonan and Shen, Han-Wei and Ritter, Alan and Machiraju, Raghu and Yen, Po-Yin},
  year = {2019},
  volume = {25},
  pages = {2181--2192},
  publisher = {{IEEE}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\CTMY3ZKR\\8667702.html},
  journal = {IEEE transactions on visualization and computer graphics},
  keywords = {Visual Analytics},
  number = {6}
}

@article{jones1972,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Jones, Karen Sparck},
  year = {1972},
  publisher = {{MCB UP Ltd}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\NDWF8NF5\\html.html},
  journal = {Journal of documentation},
  keywords = {Embeddings,NLP}
}

@article{kaski1998,
  title = {{{WEBSOM}}\textendash Self-Organizing Maps of Document Collections},
  author = {Kaski, Samuel and Honkela, Timo and Lagus, Krista and Kohonen, Teuvo},
  year = {1998},
  volume = {21},
  pages = {101--117},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\HDSTBCR7\\S0925231298000393.html},
  journal = {Neurocomputing},
  keywords = {SOM},
  number = {1-3}
}

@article{kohonen1982,
  title = {Self-Organized Formation of Topologically Correct Feature Maps},
  author = {Kohonen, Teuvo},
  year = {1982},
  volume = {43},
  pages = {59--69},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\KLCVASJ3\\BF00337288.html},
  journal = {Biological cybernetics},
  keywords = {SOM},
  number = {1}
}

@incollection{kohonen2001,
  title = {Software {{Tools}} for {{SOM}}},
  booktitle = {Self-Organizing Maps},
  author = {Kohonen, Teuvo},
  year = {2001},
  pages = {311--328},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\WRXYPH5N\\978-3-642-56927-2_8.html},
  keywords = {SOM}
}

@article{kohonen2013,
  title = {Essentials of the Self-Organizing Map},
  author = {Kohonen, Teuvo},
  year = {2013},
  volume = {37},
  pages = {52--65},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\5TE6QERW\\S0893608012002596.html},
  journal = {Neural networks},
  keywords = {SOM}
}

@inproceedings{lafia2019,
  title = {Enabling the {{Discovery}} of {{Thematically Related Research Objects}} with {{Systematic Spatializations}}},
  booktitle = {14th {{International Conference}} on {{Spatial Information Theory}} ({{COSIT}} 2019)},
  author = {Lafia, Sara and Last, Christina and Kuhn, Werner},
  year = {2019},
  publisher = {{Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\IZPH57LI\\Lafia et al. - 2019 - Enabling the Discovery of Thematically Related Res.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\9B4J3VJQ\\11110.html},
  keywords = {Visual Analytics}
}

@article{lagus1999,
  title = {Keyword Selection Method for Characterizing Text Document Maps},
  author = {Lagus, Krista and Kaski, Samuel},
  year = {1999},
  publisher = {{IET}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\9VRME94A\\Lagus and Kaski - 1999 - Keyword selection method for characterizing text d.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\FHXXA24S\\cp_19991137.html}
}

@inproceedings{le2014,
  title = {Distributed Representations of Sentences and Documents},
  booktitle = {International Conference on Machine Learning},
  author = {Le, Quoc and Mikolov, Tomas},
  year = {2014},
  pages = {1188--1196},
  publisher = {{PMLR}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\4JSL9BF7\\Le and Mikolov - 2014 - Distributed representations of sentences and docum.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\XBA6NESB\\le14.html},
  keywords = {Embeddings,NLP}
}

@article{mcinnes2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/QBY4NTED/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;/home/dsilva/Zotero/storage/VNZZEC3A/1802.html},
  journal = {arXiv:1802.03426 [cs, stat]},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@misc{moosavi2014,
  title = {{{SOMPY}}: {{A Python Library}} for {{Self Organizing Map}} ({{SOM}})},
  author = {Moosavi, Vahid and Packmann, S and Vall{\'e}s, Ivan},
  year = {2014},
  month = feb,
  abstract = {A Python Library for Self Organizing Map (SOM). Contribute to sevamoo/SOMPY development by creating an account on GitHub.},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  keywords = {SOM}
}

@article{papadimitriou2000,
  title = {Latent Semantic Indexing: {{A}} Probabilistic Analysis},
  shorttitle = {Latent Semantic Indexing},
  author = {Papadimitriou, Christos H. and Raghavan, Prabhakar and Tamaki, Hisao and Vempala, Santosh},
  year = {2000},
  volume = {61},
  pages = {217--235},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\LXNTN7QA\\S0022000000917112.html},
  journal = {Journal of Computer and System Sciences},
  keywords = {Embeddings,NLP},
  number = {2}
}

@book{schutze2008,
  title = {Introduction to Information Retrieval},
  author = {Sch{\"u}tze, Hinrich and Manning, Christopher D. and Raghavan, Prabhakar},
  year = {2008},
  volume = {39},
  publisher = {{Cambridge University Press Cambridge}},
  keywords = {IR}
}

@incollection{ultsch1993,
  title = {Self-Organizing Neural Networks for Visualisation and Classification},
  booktitle = {Information and Classification},
  author = {Ultsch, Alfred},
  year = {1993},
  pages = {307--313},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\JGFQZ38C\\978-3-642-50974-2_31.html},
  keywords = {SOM,U-Matrix}
}

@article{vandermaaten2008,
  title = {Visualizing Data Using T-{{SNE}}.},
  author = {{Van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  volume = {9},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\SJKFEZZM\\Van der Maaten and Hinton - 2008 - Visualizing data using t-SNE..pdf},
  journal = {Journal of machine learning research},
  keywords = {t-SNE},
  number = {11}
}

@book{murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  publisher = {{MIT press}},
}

@article{mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/XRZF2YA4/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/home/dsilva/Zotero/storage/4JYJF748/1301.html},
  journal = {arXiv:1301.3781 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/8ULXIGGK/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/dsilva/Zotero/storage/LUEA3YPW/1706.html},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{devlin2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/ADUJ2UGJ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/dsilva/Zotero/storage/YYX753IC/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/CSWIAXV5/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/home/dsilva/Zotero/storage/PIPH6M4G/1908.html},
  journal = {arXiv:1908.10084 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{lafia2021a,
  title = {Mapping Research Topics at Multiple Levels of Detail},
  author = {Lafia, Sara and Kuhn, Werner and Caylor, Kelly and Hemphill, Libby},
  year = {2021},
  month = mar,
  volume = {2},
  pages = {100210},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2021.100210},
  abstract = {The institutional review of interdisciplinary bodies of research lacks methods to systematically produce higher-level abstractions. Abstraction methods, like the ``distant reading'' of corpora, are increasingly important for knowledge discovery in the sciences and humanities. We demonstrate how abstraction methods complement the metrics on which research reviews currently rely. We model cross-disciplinary topics of research publications and projects emerging at multiple levels of detail in the context of an institutional review of the Earth Research Institute (ERI) at the University of California at Santa Barbara. From these, we design science maps that reveal the latent thematic structure of ERI's interdisciplinary research and enable reviewers to ``read'' a body of research at multiple levels of detail. We find that our approach provides decision support and reveals trends that strengthen the institutional review process by exposing regions of thematic expertise, distributions and clusters of work, and the evolution of these aspects.},
  journal = {Patterns},
  keywords = {data discovery,decision support,institutional review,knowledge representation,science mapping,spatialization,topic modeling},
  language = {en},
  number = {3}
}


@article{lee1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  year = {1999},
  month = oct,
  volume = {401},
  pages = {788--791},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/44565},
  abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
  copyright = {1999 Macmillan Magazines Ltd.},
  file = {/home/dsilva/Zotero/storage/DFW74B2R/Lee and Seung - 1999 - Learning the parts of objects by non-negative matr.pdf;/home/dsilva/Zotero/storage/AYLQDFU8/44565.html},
  journal = {Nature},
  language = {en},
  number = {6755}
}

