
@article{berry1995,
  title = {Using Linear Algebra for Intelligent Information Retrieval},
  author = {Berry, Michael W. and Dumais, Susan T. and O'Brien, Gavin W.},
  year = {1995},
  volume = {37},
  pages = {573--595},
  publisher = {{SIAM}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\RR58X3XG\\Berry et al. - 1995 - Using linear algebra for intelligent information r.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\K4VVY48V\\1037127.html},
  journal = {SIAM review},
  keywords = {Embeddings,NLP},
  number = {4}
}

@article{blei2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  volume = {3},
  pages = {993--1022},
  publisher = {{JMLR. org}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\PD9SIEF8\\Blei et al. - 2003 - Latent dirichlet allocation.pdf},
  journal = {the Journal of machine Learning research},
  keywords = {Embeddings,NLP}
}

@article{conneau2018,
  title = {Senteval: {{An}} Evaluation Toolkit for Universal Sentence Representations},
  shorttitle = {Senteval},
  author = {Conneau, Alexis and Kiela, Douwe},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1803.05449},
  eprinttype = {arxiv},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\68VAVMDP\\Conneau and Kiela - 2018 - Senteval An evaluation toolkit for universal sent.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\QISHR4LV\\1803.html},
  journal = {arXiv preprint arXiv:1803.05449},
  keywords = {Embeddings}
}

@article{conneau2019,
  title = {Unsupervised Cross-Lingual Representation Learning at Scale},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  archiveprefix = {arXiv},
  eprint = {1911.02116},
  eprinttype = {arxiv},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\FD93ZRR6\\Conneau et al. - 2019 - Unsupervised cross-lingual representation learning.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\GJ3JEUBN\\1911.html},
  journal = {arXiv preprint arXiv:1911.02116},
  keywords = {BERT,Embeddings,NLP}
}

@article{deerwester1990,
  title = {Indexing by Latent Semantic Analysis},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  year = {1990},
  volume = {41},
  pages = {391--407},
  publisher = {{Wiley Online Library}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\YSPY2TKF\\Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\3QVT4UJ4\\(SICI)1097-4571(199009)416391AID-ASI13.0.html},
  journal = {Journal of the American society for information science},
  keywords = {Embeddings,NLP},
  number = {6}
}

@inproceedings{dumais1988,
  title = {Using Latent Semantic Analysis to Improve Access to Textual Information},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on {{Human}} Factors in Computing Systems},
  author = {Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Deerwester, Scott and Harshman, Richard},
  year = {1988},
  pages = {281--285},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\8XIGQ3EM\\Dumais et al. - 1988 - Using latent semantic analysis to improve access t.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\VX9F8AVP\\57167.html},
  keywords = {Embeddings,NLP}
}

@article{harris1954,
  title = {Distributional Structure},
  author = {Harris, Zellig S.},
  year = {1954},
  volume = {10},
  pages = {146--162},
  publisher = {{Taylor \& Francis}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\99EYM5II\\Harris - 1954 - Distributional structure.pdf},
  journal = {Word},
  keywords = {Embeddings,NLP},
  number = {2-3}
}

@article{henriques2012,
  title = {Exploratory Geospatial Data Analysis Using the {{GeoSOM}} Suite},
  author = {Henriques, Roberto and Bacao, Fernando and Lobo, Victor},
  year = {2012},
  volume = {36},
  pages = {218--232},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\6P28EDC6\\S0198971511001141.html},
  journal = {Computers, Environment and Urban Systems},
  keywords = {SOM},
  number = {3}
}

@inproceedings{hofmann1999,
  title = {Probabilistic Latent Semantic Indexing},
  booktitle = {Proceedings of the 22nd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Hofmann, Thomas},
  year = {1999},
  pages = {50--57},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\YY2CTWDC\\Hofmann - 1999 - Probabilistic latent semantic indexing.pdf},
  keywords = {Embeddings,NLP}
}

@inproceedings{hu2008,
  title = {Collaborative Filtering for Implicit Feedback Datasets},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
  year = {2008},
  pages = {263--272},
  publisher = {{Ieee}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\2MSUVD8I\\4781121.html},
  keywords = {Recommender}
}

@article{ji2019,
  title = {Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection},
  shorttitle = {Visual Exploration of Neural Document Embedding in Information Retrieval},
  author = {Ji, Xiaonan and Shen, Han-Wei and Ritter, Alan and Machiraju, Raghu and Yen, Po-Yin},
  year = {2019},
  volume = {25},
  pages = {2181--2192},
  publisher = {{IEEE}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\CTMY3ZKR\\8667702.html},
  journal = {IEEE transactions on visualization and computer graphics},
  keywords = {Visual Analytics},
  number = {6}
}

@article{jones1972,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Jones, Karen Sparck},
  year = {1972},
  publisher = {{MCB UP Ltd}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\NDWF8NF5\\html.html},
  journal = {Journal of documentation},
  keywords = {Embeddings,NLP}
}

@article{kaski1998,
  title = {{{WEBSOM}}\textendash Self-Organizing Maps of Document Collections},
  author = {Kaski, Samuel and Honkela, Timo and Lagus, Krista and Kohonen, Teuvo},
  year = {1998},
  volume = {21},
  pages = {101--117},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\HDSTBCR7\\S0925231298000393.html},
  journal = {Neurocomputing},
  keywords = {SOM},
  number = {1-3}
}

@article{kohonen1982,
  title = {Self-Organized Formation of Topologically Correct Feature Maps},
  author = {Kohonen, Teuvo},
  year = {1982},
  volume = {43},
  pages = {59--69},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\KLCVASJ3\\BF00337288.html},
  journal = {Biological cybernetics},
  keywords = {SOM},
  number = {1}
}

@incollection{kohonen2001,
  title = {Software {{Tools}} for {{SOM}}},
  booktitle = {Self-Organizing Maps},
  author = {Kohonen, Teuvo},
  year = {2001},
  pages = {311--328},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\WRXYPH5N\\978-3-642-56927-2_8.html},
  keywords = {SOM}
}

@article{kohonen2013,
  title = {Essentials of the Self-Organizing Map},
  author = {Kohonen, Teuvo},
  year = {2013},
  volume = {37},
  pages = {52--65},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\5TE6QERW\\S0893608012002596.html},
  journal = {Neural networks},
  keywords = {SOM}
}

@inproceedings{lafia2019,
  title = {Enabling the {{Discovery}} of {{Thematically Related Research Objects}} with {{Systematic Spatializations}}},
  booktitle = {14th {{International Conference}} on {{Spatial Information Theory}} ({{COSIT}} 2019)},
  author = {Lafia, Sara and Last, Christina and Kuhn, Werner},
  year = {2019},
  publisher = {{Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\IZPH57LI\\Lafia et al. - 2019 - Enabling the Discovery of Thematically Related Res.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\9B4J3VJQ\\11110.html},
  keywords = {Visual Analytics}
}

@article{lagus1999,
  title = {Keyword Selection Method for Characterizing Text Document Maps},
  author = {Lagus, Krista and Kaski, Samuel},
  year = {1999},
  publisher = {{IET}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\9VRME94A\\Lagus and Kaski - 1999 - Keyword selection method for characterizing text d.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\FHXXA24S\\cp_19991137.html}
}

@inproceedings{le2014,
  title = {Distributed Representations of Sentences and Documents},
  booktitle = {International Conference on Machine Learning},
  author = {Le, Quoc and Mikolov, Tomas},
  year = {2014},
  pages = {1188--1196},
  publisher = {{PMLR}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\4JSL9BF7\\Le and Mikolov - 2014 - Distributed representations of sentences and docum.pdf;C\:\\Users\\dsilva\\Zotero\\storage\\XBA6NESB\\le14.html},
  keywords = {Embeddings,NLP}
}

@article{mcinnes2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/QBY4NTED/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;/home/dsilva/Zotero/storage/VNZZEC3A/1802.html},
  journal = {arXiv:1802.03426 [cs, stat]},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@misc{moosavi2014,
  title = {{{SOMPY}}: {{A Python Library}} for {{Self Organizing Map}} ({{SOM}})},
  author = {Moosavi, Vahid and Packmann, S and Vall{\'e}s, Ivan},
  year = {2014},
  month = feb,
  abstract = {A Python Library for Self Organizing Map (SOM). Contribute to sevamoo/SOMPY development by creating an account on GitHub.},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  keywords = {SOM}
}

@article{papadimitriou2000,
  title = {Latent Semantic Indexing: {{A}} Probabilistic Analysis},
  shorttitle = {Latent Semantic Indexing},
  author = {Papadimitriou, Christos H. and Raghavan, Prabhakar and Tamaki, Hisao and Vempala, Santosh},
  year = {2000},
  volume = {61},
  pages = {217--235},
  publisher = {{Elsevier}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\LXNTN7QA\\S0022000000917112.html},
  journal = {Journal of Computer and System Sciences},
  keywords = {Embeddings,NLP},
  number = {2}
}

@book{schutze2008,
  title = {Introduction to Information Retrieval},
  author = {Sch{\"u}tze, Hinrich and Manning, Christopher D. and Raghavan, Prabhakar},
  year = {2008},
  volume = {39},
  publisher = {{Cambridge University Press Cambridge}},
  keywords = {IR}
}

@incollection{ultsch1993,
  title = {Self-Organizing Neural Networks for Visualisation and Classification},
  booktitle = {Information and Classification},
  author = {Ultsch, Alfred},
  year = {1993},
  pages = {307--313},
  publisher = {{Springer}},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\JGFQZ38C\\978-3-642-50974-2_31.html},
  keywords = {SOM,U-Matrix}
}

@article{vandermaaten2008,
  title = {Visualizing Data Using T-{{SNE}}.},
  author = {{Van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  volume = {9},
  file = {C\:\\Users\\dsilva\\Zotero\\storage\\SJKFEZZM\\Van der Maaten and Hinton - 2008 - Visualizing data using t-SNE..pdf},
  journal = {Journal of machine learning research},
  keywords = {t-SNE},
  number = {11}
}

@book{murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  publisher = {{MIT press}},
}

@article{mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/XRZF2YA4/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/home/dsilva/Zotero/storage/4JYJF748/1301.html},
  journal = {arXiv:1301.3781 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/8ULXIGGK/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/dsilva/Zotero/storage/LUEA3YPW/1706.html},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{devlin2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/ADUJ2UGJ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/dsilva/Zotero/storage/YYX753IC/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT}}-{{Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  eprint = {1908.10084},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/CSWIAXV5/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/home/dsilva/Zotero/storage/PIPH6M4G/1908.html},
  journal = {arXiv:1908.10084 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@article{lafia2021a,
  title = {Mapping Research Topics at Multiple Levels of Detail},
  author = {Lafia, Sara and Kuhn, Werner and Caylor, Kelly and Hemphill, Libby},
  year = {2021},
  month = mar,
  volume = {2},
  pages = {100210},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2021.100210},
  abstract = {The institutional review of interdisciplinary bodies of research lacks methods to systematically produce higher-level abstractions. Abstraction methods, like the ``distant reading'' of corpora, are increasingly important for knowledge discovery in the sciences and humanities. We demonstrate how abstraction methods complement the metrics on which research reviews currently rely. We model cross-disciplinary topics of research publications and projects emerging at multiple levels of detail in the context of an institutional review of the Earth Research Institute (ERI) at the University of California at Santa Barbara. From these, we design science maps that reveal the latent thematic structure of ERI's interdisciplinary research and enable reviewers to ``read'' a body of research at multiple levels of detail. We find that our approach provides decision support and reveals trends that strengthen the institutional review process by exposing regions of thematic expertise, distributions and clusters of work, and the evolution of these aspects.},
  journal = {Patterns},
  keywords = {data discovery,decision support,institutional review,knowledge representation,science mapping,spatialization,topic modeling},
  language = {en},
  number = {3}
}


@article{lee1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  year = {1999},
  month = oct,
  volume = {401},
  pages = {788--791},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/44565},
  abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
  copyright = {1999 Macmillan Magazines Ltd.},
  file = {/home/dsilva/Zotero/storage/DFW74B2R/Lee and Seung - 1999 - Learning the parts of objects by non-negative matr.pdf;/home/dsilva/Zotero/storage/AYLQDFU8/44565.html},
  journal = {Nature},
  language = {en},
  number = {6755}
}

@techreport{calof2017,
  title = {Competitive {{Intelligence}}: {{A}} 10-Year {{Global Development}}},
  author = {Calof, Jonathan and Sewdass, Nisha and Arcos, Rub{\'e}n},
  year = {2017},
  institution = {{Competitive Intelligence Foundation}}
}

@article{marin2004,
  title = {Dissemination of {{Competitive Intelligence}}},
  author = {Marin, Jane and Poulter, Alan},
  year = {2004},
  month = apr,
  volume = {30},
  pages = {165--180},
  publisher = {{SAGE Publications Ltd}},
  issn = {0165-5515},
  doi = {10.1177/0165551504042806},
  abstract = {The paper argues that competitive intelligence is a vital function and attempts to study how it is distributed, especially by technologies, within organizations. Related topics, the sources of competitive intelligence, and who distributes and receives competitive intelligence, are also addressed. A literature-based study is extended by a quantitative survey of members of the Society of Competitive Information Professionals (SCIP) and email interviews with a self-chosen sample of respondees. The paper concludes that the distribution of competitive intelligence can be aided by technology but to be effective must be primarily `person-focused'. Competitive intelligence itself needs to be seen as a form of knowledge management rather than an information provision function. This has implications for sources and for the professional roots of those who provide competitive intelligence. Evaluation of competitive information provision is seen as a next step for research.},
  file = {/home/dsilva/Zotero/storage/PM6YEY32/Marin and Poulter - 2004 - Dissemination of Competitive Intelligence.pdf},
  journal = {Journal of Information Science},
  keywords = {competitive intelligence,competitive intelligence professionals,data analysis,evaluation,information dissemination,knowledge management,library and information professionals,personal communication,skills,technology use,usage,users},
  language = {en},
  number = {2}
}

@article{brod1999,
  title={Competitive Intelligence: Harvesting information to compete and market intelligently},
  author={Brod, Susan},
  journal={Camares Communications, New York, NY},
  year={1999}
}

@inproceedings{dey2011,
  title = {Acquiring Competitive Intelligence from Social Media},
  booktitle = {Proceedings of the 2011 {{Joint Workshop}} on {{Multilingual OCR}} and {{Analytics}} for {{Noisy Unstructured Text Data}}},
  author = {Dey, Lipika and Haque, Sk. Mirajul and Khurdiya, Arpit and Shroff, Gautam},
  year = {2011},
  month = sep,
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2034617.2034621},
  abstract = {Competitive intelligence is the art of defining, gathering and analyzing intelligence about competitor's products, promotions, sales etc. from external sources. The Web comes across as an important source for gathering competitive intelligence. News, blogs, as well as social media not only provide competitors information but also provide direct comparison of customer behaviors with respect to different verticals among competing organizations. This paper discusses methodologies to obtain competitive intelligence from different types of web resources including social media using a wide array of text mining techniques. It provides some results from case-studies to show how the gathered information can be integrated with structured data and used to explain business facts and thereby adopted for future decision making.},
  isbn = {978-1-4503-0685-0},
  keywords = {business intelligence from social media,competitive intelligence,content analysis,text mining},
  series = {{{MOCR}}\_{{AND}} '11}
}

@article{malkov2018,
  title = {Efficient and Robust Approximate Nearest Neighbor Search Using {{Hierarchical Navigable Small World}} Graphs},
  author = {Malkov, Yu A. and Yashunin, D. A.},
  year = {2018},
  month = aug,
  abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
  archiveprefix = {arXiv},
  eprint = {1603.09320},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/X2MQEFIW/Malkov and Yashunin - 2018 - Efficient and robust approximate nearest neighbor .pdf;/home/dsilva/Zotero/storage/P5KKSX5L/1603.html},
  journal = {arXiv:1603.09320 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Data Structures and Algorithms,Computer Science - Information Retrieval,Computer Science - Social and Information Networks},
  primaryclass = {cs}
}

@article{bajaj2018,
  title = {{{MS MARCO}}: {{A Human Generated MAchine Reading COmprehension Dataset}}},
  shorttitle = {{{MS MARCO}}},
  author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
  year = {2018},
  month = oct,
  abstract = {We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
  archiveprefix = {arXiv},
  eprint = {1611.09268},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/GWRZSQ6M/Bajaj et al. - 2018 - MS MARCO A Human Generated MAchine Reading COmpre.pdf;/home/dsilva/Zotero/storage/6KBI773E/1611.html},
  journal = {arXiv:1611.09268 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  primaryclass = {cs}
}

@article{nogueira2020a,
  title = {Passage {{Re}}-Ranking with {{BERT}}},
  author = {Nogueira, Rodrigo and Cho, Kyunghyun},
  year = {2020},
  month = apr,
  abstract = {Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27\% (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/dl4marco-bert},
  archiveprefix = {arXiv},
  eprint = {1901.04085},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/VIB9A7BZ/Nogueira and Cho - 2020 - Passage Re-ranking with BERT.pdf;/home/dsilva/Zotero/storage/HRA2DF28/1901.html},
  journal = {arXiv:1901.04085 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryclass = {cs}
}

@article{kratzwald2019,
  title = {{{RankQA}}: {{Neural Question Answering}} with {{Answer Re}}-{{Ranking}}},
  shorttitle = {{{RankQA}}},
  author = {Kratzwald, Bernhard and Eigenmann, Anna and Feuerriegel, Stefan},
  year = {2019},
  month = aug,
  abstract = {The conventional paradigm in neural question answering (QA) for narrative content is limited to a two-stage process: first, relevant text passages are retrieved and, subsequently, a neural network for machine comprehension extracts the likeliest answer. However, both stages are largely isolated in the status quo and, hence, information from the two phases is never properly fused. In contrast, this work proposes RankQA: RankQA extends the conventional two-stage process in neural QA with a third stage that performs an additional answer re-ranking. The re-ranking leverages different features that are directly extracted from the QA pipeline, i.e., a combination of retrieval and comprehension features. While our intentionally simple design allows for an efficient, data-sparse estimation, it nevertheless outperforms more complex QA systems by a significant margin: in fact, RankQA achieves state-of-the-art performance on 3 out of 4 benchmark datasets. Furthermore, its performance is especially superior in settings where the size of the corpus is dynamic. Here the answer re-ranking provides an effective remedy against the underlying noise-information trade-off due to a variable corpus size. As a consequence, RankQA represents a novel, powerful, and thus challenging baseline for future research in content-based QA.},
  archiveprefix = {arXiv},
  eprint = {1906.03008},
  eprinttype = {arxiv},
  file = {/home/dsilva/Zotero/storage/8U4QZV24/Kratzwald et al. - 2019 - RankQA Neural Question Answering with Answer Re-R.pdf;/home/dsilva/Zotero/storage/IVMHY3X7/1906.html},
  journal = {arXiv:1906.03008 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}