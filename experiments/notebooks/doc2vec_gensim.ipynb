{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description:\n",
    "In this notebook we request 100 news articles from the NewsAPI (maximum allowed) and we use their truncated content to build a corpus, then we preprocess the corpus using the built CorpusPreprocess scikit-learn-like transformer. We train a gensim doc2vec model on the preprocessed corpus and we assess the model by checking document rankings for each document (the document should be the most similar with itself) and by comparing random documents' content with their similar documents' content. Finally, we request new documents from NewsAPI, apply preprocessing, infer their vectors and assess their quality by getting their most similar documents.\n",
    "\n",
    "# TODO:\n",
    "- add date, price, weekday, ... token to CorpusPreprocess\n",
    "- webscrape full content from urls provided by api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.embedding_eval import compare_documents\n",
    "from src.data.text_preprocessing import CorpusPreprocess\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import collections\n",
    "from newsapi import NewsApiClient\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import models\n",
    "# import numpy as np\n",
    "# from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# if running from container\n",
    "if dotenv_path == '':\n",
    "    dotenv_path = \"/run/secrets/dotenv-file\"  # hard-coded: path to secret passed through docker-compose\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "NEWSAPIKEY = os.environ.get(\"NEWSAPIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of article content:\n",
      "\n",
      "The 49ers two violations of the offseason resulted in fines but no cancellations of the teams workouts, sources told NBC Sports Bay Area.\r\n",
      "The 49ers engaged in two activities during their voluntary o… [+2823 chars]\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "newsapi = NewsApiClient(api_key=NEWSAPIKEY)\n",
    "\n",
    "# Get news articles\n",
    "articles = newsapi.get_top_headlines(language='en',\n",
    "                                     category='sports',  # 'business','entertainment','general','health','science','sports','technology'\n",
    "                                      # domains='bbc.co.uk',\n",
    "                                      # from_param=datetime.today() - timedelta(30),\n",
    "                                      # to=datetime.today(),\n",
    "                                      page_size=100,\n",
    "                                      country='us')\n",
    "\n",
    "corpus = list(set([c['content'] for c in articles['articles'] if c['content']]))\n",
    "\n",
    "print(\"Example of article content:\\n\\n{}\".format(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/ test split\n",
    "test_idx = random.sample(range(len(corpus)), int(len(corpus) * 0.1))\n",
    "test_corpus = [corpus[i] for i in test_idx]\n",
    "train_corpus = list(set(corpus).difference(set(test_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of preprocessed article content:\n",
      "\n",
      "['49er', 'two', 'violat', 'offseason', 'result', 'fine', 'cancel', 'team', 'workout', 'sourc', 'told', 'nbc', 'sport', 'bay', 'area', '49er', 'engag', 'two', 'activ', 'dure', 'voluntari', '2823', 'char']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing - removing stopwords, lowercasing, strip accents, strip punctuation, stemming, max_df and min_df thresholds\n",
    "prep = CorpusPreprocess(stop_words=stopwords.words('english'), lowercase=True, strip_accents=True,\n",
    "                        strip_punctuation=punctuation, stemmer=PorterStemmer(), max_df=0.2, min_df=2)\n",
    "processed_train_corpus = prep.fit_transform(train_corpus)\n",
    "processed_test_corpus = prep.transform(test_corpus)\n",
    "\n",
    "print(\"Example of preprocessed article content:\\n\\n{}\".format(processed_train_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaggedDocument format (input to doc2vec)\n",
    "tagged_corpus = [models.doc2vec.TaggedDocument(text, [i]) for i, text in enumerate(processed_train_corpus)]\n",
    "\n",
    "# Doc2Vec model\n",
    "model = models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=200)\n",
    "model.build_vocab(tagged_corpus)\n",
    "model.train(tagged_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['49er', 'two', 'offseason', 'team', 'sport', 'bay', 'dure', 'char', 'dechambeau', 'three', 'everi', 'ha', 'one', 'point', 'shot', 'friday', '2020', 'uefa', 'european', 'championship', 'semifin', '21', 'win', 'saturday', 'way', 'offer', 'chanc', 'opportun', 'court', 'avail', 'full', 'first', 'time', 'wa', 'posit', 'go', 'thi', 'next', 'month', 'right', 'detroit', '14', 'nativ', 'keep', 'trophi', 'second', 'attempt', 'sun', 'mark', 'score', 'half', 'ufc', 'poirier', 'last', 'star', 'philadelphia', '2021', 'nfl', 'season', 'train', 'camp', 'quickli', 'philli', 'make', 'year', 'even', 'red', 'sox', 'look', 'straight', 'tonight', 'behind', 'third', 'ball', 'final', 'settl', 'chicago', 'welcom', 'comment', 'stori', 'sign', 'minor', 'leagu', 'contract', 'week', 'announc', 'pitch', 'mizzou', 'footbal', 'talk', 'lake', 'st', 'loui', 'thursday', 'atlanta', 'north', 'citi', 'five', 'rule', 'test', 'hi', 'mlb', 'start', 'busch', 'monday', 'robert', 'milwauke', 'buck', 'nba', 'champion', 'eastern', 'confer', 'play', 'world', 'top', 'player', 'advanc', 'wimbledon', 'sinc', 'would', 'appear', 'seri', 'lot', 'presid', 'biden', 'olymp', 'day', 'away', 'best', 'basebal', 'lead', 'trade', 'theyr', 'talent', 'roster', 'like', 'race', 'athlet', 'took', 'late', 'giant', 'made', 'sens', 'onli', 'back', 'enter', '30', 'game', 'dodson', 'cleveland', 'hit', 'crowd', 'run', 'major', 'hot', 'dog', 'might', 'nathan', 'roger', 'feder', 'pittsburgh', 'count', 'stretch', 'thorn', 'midfield', 'penn', 'state', 'lineback', 'wyli', 'mani', 'held', 'steeler', 'order'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab.keys()  # this accesses the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(0, 43), (1, 6), (2, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Assessing Doc2Vec model\n",
    "ranks = []\n",
    "for doc_id in range(len(tagged_corpus)):\n",
    "    inferred_vector = model.infer_vector(tagged_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "# Optimally we want as much documents to be the most similar with themselves (i.e. rank 0)\n",
    "print(collections.OrderedDict(sorted(collections.Counter(ranks).items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "Above we can the distribution of self-document similarity rank (i.e. ~ 53 documents have itself as the most similar document - rank 0, ~ 2 documents have itself as the second most similar document - rank 0, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (3): «Reggie Bush wants his Heisman Trophy back. \r\n",
      "That was made very clear in a statement posted to his social media account Thursday.\r\n",
      "\"Over the last few months, on multiple occasions, my team and I have… [+2861 chars]»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS:\n",
      "MOST (28, 0.9008356332778931): «John Dodson has made his first comments on Friday’s serious car accident that nearly took his life as well as those of his wife and three children.\r\n",
      "Dodson, a 17-fight UFC veteran and Season 14 winne… [+2357 chars]»\n",
      "\n",
      "SECOND-MOST (13, 0.8890371322631836): «The Cubs signed Tony Cingrani to a minor league contract earlier this week, per an announcement from the Lexington Legends of the Atlantic League. The southpaw had been pitching for the independent c… [+987 chars]»\n",
      "\n",
      "MEDIAN (43, 0.8203971982002258): «The Tampa Bay Lightning are one win away from sweeping the Stanley Cup Final, but if they clinch the series in Game 4 on Monday, families of players and staff will not be on the ice to celebrate.\r\n",
      "Th… [+3085 chars]»\n",
      "\n",
      "LEAST (34, 0.25823265314102173): «If you're like a lot of hot dog lovers, you might feel there's just nothing like Nathan's. The only problem with a good hot dog is that it might leave you wanting more. Be careful: With Nathan's Famo… [+3711 chars]»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity between random test doc and train docs\n",
    "base_doc_id = random.choice(range(len(test_corpus)))\n",
    "inferred_unknown_vector = model.infer_vector(processed_test_corpus[base_doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_unknown_vector], topn=model.docvecs.count)\n",
    "compare_out = compare_documents(base_doc_id, test_corpus[base_doc_id], sims, train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (2): «Christian Eriksen remains in hospital in a stable condition\r\n",
      "The decision to resume Denmark's Euro 2020 opener against Finland following Christian Eriksen's cardiac arrest was the \"least bad one\", sa… [+3752 chars]»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS:\n",
      "MOST (16, 0.9594348073005676): «Italy left-back Leonardo Spinazzola has been ruled out of Euro 2020 after tests on Saturday confirmed he had suffered a torn Achilles tendon.\r\n",
      "The 28-year-old has been one of the standout performers … [+2400 chars]»\n",
      "\n",
      "SECOND-MOST (2, 0.9555896520614624): «Denmark are into the 2020 UEFA European Championship semifinals after a 2-1 win over the Czech Republic in Baku, Azerbaijan, on Saturday.\r\n",
      "Thomas Delaney and Kasper Dolberg goals were enough for the … [+1112 chars]»\n",
      "\n",
      "MEDIAN (20, 0.8771800994873047): «It’s been almost 24 hours since we found out the Dallas Cowboys would be appearing on HBO’s “Hard Knocks” series in 2021. Since then there have been a lot of misconceptions and even some misinformati… [+3470 chars]»\n",
      "\n",
      "LEAST (34, 0.5527355670928955): «If you're like a lot of hot dog lovers, you might feel there's just nothing like Nathan's. The only problem with a good hot dog is that it might leave you wanting more. Be careful: With Nathan's Famo… [+3711 chars]»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get new news articles\n",
    "new_articles = newsapi.get_everything(language='en',\n",
    "                                      domains='bbc.co.uk',\n",
    "                                      from_param=datetime.today() - timedelta(30),\n",
    "                                      to=datetime.today() - timedelta(20),\n",
    "                                      page_size=10)\n",
    "\n",
    "new_corpus = list(set([c['content'] for c in new_articles['articles'] if c['content']]))\n",
    "\n",
    "# Apply preprocessing\n",
    "new_processed_corpus = prep.transform(new_corpus)\n",
    "\n",
    "# Similarity query\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "unkwnown_doc = new_processed_corpus[doc_id]\n",
    "inferred_unknown_vector = model.infer_vector(unkwnown_doc)\n",
    "sims = model.docvecs.most_similar([inferred_unknown_vector], topn=model.docvecs.count)\n",
    "compare_out = compare_documents(doc_id, new_corpus[doc_id], sims, train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
