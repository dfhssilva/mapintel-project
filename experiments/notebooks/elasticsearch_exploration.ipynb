{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description:\n",
    "In this notebook we explore the MongoDB instance where we keep our news documents. We see some examples of documents stored, the time at which each batch of documents was inserted, provide a function to remove batch of documents given their insertion time, analyze the distribution of time at which each document is published, analyze the distribution of category of documents as well as their sources, analyze the relationship between category and source, provide a function to remove documents with no content or description and a function to check/ remove duplicate documents based on description and content combined. This function was used previoulsy the creation of the unique index of content and description. With this index there shouldn't be any duplicate documents.\n",
    "\n",
    "# TODO:\n",
    "- UPDATE THIS TO GET DATA FROM ELASTICSEARCH OVER MONGODB   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/http_urllib3.py:206: UserWarning: Connecting to https://odfe-node1:9200 using SSL with verify_certs=False is insecure.\n",
      "  % self.host\n",
      "/usr/local/lib/python3.7/dist-packages/elasticsearch/connection/http_urllib3.py:206: UserWarning: Connecting to https://0.0.0.0:9200 using SSL with verify_certs=False is insecure.\n",
      "  % self.host\n"
     ]
    }
   ],
   "source": [
    "# Connecting to Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    hosts=['odfe-node1', '0.0.0.0'],\n",
    "    http_auth=('admin', 'admin'),\n",
    "    scheme=\"https\",\n",
    "    verify_certs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:1006: InsecureRequestWarning: Unverified HTTPS request is being made to host 'odfe-node1'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'security-auditlog-2021.08.01': {'aliases': {}},\n",
       " 'security-auditlog-2021.08.02': {'aliases': {}},\n",
       " 'label': {'aliases': {}},\n",
       " 'security-auditlog-2021.07.31': {'aliases': {}},\n",
       " 'security-auditlog-2021.08.03': {'aliases': {}},\n",
       " '.opendistro_security': {'aliases': {}},\n",
       " 'document': {'aliases': {}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List indices\n",
    "es.indices.get_alias(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A database is compose of collections. These collections in turn hold documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_328/1472291047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollection_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_collection_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The database contains {len(collection_list)} collections: {collection_list[:5]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "collection_list = db.list_collection_names()\n",
    "print(f\"The database contains {len(collection_list)} collections: {collection_list[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in collection_list:\n",
    "    print(f\"Collection {col} contains {db[col].count_documents({})} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in collection_list:\n",
    "    print(f\"EXAMPLE OF {col.upper()}'S DOCUMENTS:\")\n",
    "    pprint(db[col].find_one())\n",
    "    print(\"-------------------------------------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_batches = [\n",
    "    {  # project id_timestamp\n",
    "        '$project': {\n",
    "            'id_timestamp': {\n",
    "                '$dateToString': {\n",
    "                    'format': '%d-%m-%Y T%H:%M:%S', \n",
    "                    'date': {'$toDate': '$_id'}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {  # groups on id_timestamp and counts the number of documents for each values\n",
    "        '$group': {\n",
    "            '_id': '$id_timestamp',\n",
    "            'document_count': {'$sum': 1}\n",
    "        }\n",
    "    },\n",
    "    {  # sort results in descending order by _id\n",
    "        '$sort': {'_id': -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "for col in collection_list:\n",
    "    batches = db[col].aggregate(pipeline_batches)\n",
    "    print(f\"\\n{col} collection insertion batches history:\")\n",
    "    pprint(list(batches))\n",
    "    print(\"-----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_batch_date(batch_date, db, collection=None): \n",
    "    \"\"\"\n",
    "    Function to remove documents with a given batch date from db's specified collection or all of them (default).\n",
    "    batch_date should have '%d-%m-%Y T%H:%M:%S' format.\n",
    "    \"\"\"\n",
    "    pipeline_remove = [\n",
    "        {  # project id_timestamp\n",
    "            '$project': {\n",
    "                '_id': 1,\n",
    "                'id_timestamp': {\n",
    "                    '$dateToString': {\n",
    "                        'format': '%d-%m-%Y T%H:%M:%S', \n",
    "                        'date': {'$toDate': '$_id'}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {  # match id_timestamp to batch_date we want to remove\n",
    "            '$match': {'id_timestamp': batch_date}\n",
    "        },\n",
    "        {  # project just _id\n",
    "            '$project': {'_id': 1}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if collection is None:\n",
    "        collection_list = db.list_collection_names()\n",
    "        for col in collection_list:\n",
    "            idsList = list(map(lambda x: x['_id'], db[col].aggregate(pipeline_remove)))\n",
    "            db[col].delete_many({'_id': {'$in': idsList}})\n",
    "            print(f\"{len(idsList)} documents with batch_date {batch_date} were removed from {col}\\n\")\n",
    "    else:\n",
    "        idsList = list(map(lambda x: x['_id'], db[collection].aggregate(pipeline_remove)))\n",
    "        db[collection].delete_many({'_id': {'$in': idsList}})\n",
    "        print(f\"{len(idsList)} documents with batch_date {batch_date} were removed from {collection}\\n\")\n",
    "\n",
    "# remove_by_batch_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### publishedAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {  # project publishedAtDay and publishedAtHour\n",
    "        '$project': {\n",
    "            'publishedAtDay': {\n",
    "                '$dateToString': {\n",
    "                    'format': '%d-%m-%YT%H', \n",
    "                    'date': {'$toDate': '$publishedAt'}\n",
    "                }\n",
    "            },\n",
    "            'publishedAtHour': {\n",
    "                '$hour': {\n",
    "                    'date': {'$toDate': '$publishedAt'}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {  # groups on publishedAtDay and gets number of documents per day and hour (document_count) and publishedAtHour\n",
    "        '$group': {\n",
    "            '_id': '$publishedAtDay',\n",
    "            'document_count': {'$sum': 1},\n",
    "            'publishedAtHour': {'$first': '$publishedAtHour'}\n",
    "        }\n",
    "    },\n",
    "    {  # groups on publishedAtHour and gets average of documents per hour over days\n",
    "        '$group': {\n",
    "            '_id': '$publishedAtHour',\n",
    "            'avg_document_count': {'$avg': '$document_count'}\n",
    "        }\n",
    "    },\n",
    "    {  # sort results in descending order by _id\n",
    "        '$sort': {'_id': -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(13, 9))\n",
    "for ax, col in zip(axes.flatten(), collection_list):\n",
    "    x, y = [], []\n",
    "    for i in list(db[col].aggregate(pipeline)):\n",
    "        x.append(i['_id'])\n",
    "        y.append(i['avg_document_count'])\n",
    "    ax.plot(x, y, linestyle=\"-\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title(f\"Average number of documents per publishedAt Hour - {col} collection\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {  # project category\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'category': 1\n",
    "        }\n",
    "    },\n",
    "    {  # groups on category and gets number of documents for each category\n",
    "        '$group': {\n",
    "            '_id': '$category',\n",
    "            'document_count': {'$sum': 1},\n",
    "        }\n",
    "    },\n",
    "    {  # sort results in descending order by _id\n",
    "        '$sort': {'_id': 1}\n",
    "    }\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(13, 9))\n",
    "for ax, col in zip(axes.flatten(), collection_list):\n",
    "    x, y = [], []\n",
    "    for i in list(db[col].aggregate(pipeline)):\n",
    "        x.append(i['_id'])\n",
    "        y.append(i['document_count'])\n",
    "    ax.bar(x, y)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_title(f\"Number of documents per category - {col} collection\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {  # project source\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'source': 1\n",
    "        }\n",
    "    },\n",
    "    {  # groups on source and gets number of documents for each source\n",
    "        '$group': {\n",
    "            '_id': '$source',\n",
    "            'document_count': {'$sum': 1},\n",
    "        }\n",
    "    },\n",
    "    {  # sort results in descending order by _id\n",
    "        '$sort': {'_id': 1}\n",
    "    }\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(19, 9))\n",
    "for ax, col in zip(axes.flatten(), collection_list):\n",
    "    x, y = [], []\n",
    "    for i in list(db[col].aggregate(pipeline)):\n",
    "        if i['_id'] is None:\n",
    "            x.append(\"Null\")\n",
    "        else:\n",
    "            x.append(i['_id'])\n",
    "        y.append(i['document_count'])\n",
    "    ax.bar(x, y)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x, rotation=30, ha='right')\n",
    "    ax.set_title(f\"Number of documents per source - {col} collection\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relationship between categories and sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stacked bar chart\n",
    "pipeline = [\n",
    "    {  # project source and category\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'source': 1,\n",
    "            'category': 1\n",
    "        }\n",
    "    },\n",
    "    {  # groups on source and gets number of documents for each source\n",
    "        '$group': {\n",
    "            '_id': {\n",
    "                'category': '$category',\n",
    "                'source': '$source'\n",
    "            },\n",
    "            'document_count': {'$sum': 1},\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project':{\n",
    "            '_id': 0,\n",
    "            'document_count': 1,\n",
    "            'category': '$_id.category',\n",
    "            'source': '$_id.source'            \n",
    "        }\n",
    "    },\n",
    "    {  # sort results in descending order by _id\n",
    "        '$sort': {'category': 1}\n",
    "    }\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(19, 11))\n",
    "for ax, col in zip(axes.flatten(), collection_list):\n",
    "    plt_data = pd.DataFrame(list(db[col].aggregate(pipeline))).pivot(index=\"source\", columns=\"category\", values=\"document_count\")\n",
    "    plt_data[\"sum\"] = plt_data.sum(axis=1)\n",
    "    plt_data.sort_values(\"sum\", ascending=False).drop(\"sum\", axis=1).plot(kind='bar', stacked=True, rot=90, ax=ax)\n",
    "    ax.set_title(f\"Source frequencies by category - {col} collection\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_values(db, collection=None): \n",
    "    \"\"\"\n",
    "    Function to remove documents with missing values on both description and content from db's specified collection\n",
    "    or all of them (default).\n",
    "    \"\"\"\n",
    "    pipeline_remove = [\n",
    "        {\n",
    "            '$project': {\n",
    "                '_id': 1,\n",
    "                'description': 1,\n",
    "                'content': 1\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$match\": {\n",
    "                '$or': [\n",
    "                    {\n",
    "                        \"description\" : {\"$eq\" : None},\n",
    "                        \"content\" : {\"$eq\": None}\n",
    "                    },\n",
    "                    {\n",
    "                        \"description\" : {\"$eq\" : ''},\n",
    "                        \"content\" : {\"$eq\": ''}\n",
    "                    },\n",
    "                    {\n",
    "                        \"description\" : {\"$eq\" : None},\n",
    "                        \"content\" : {\"$eq\": ''}\n",
    "                    },\n",
    "                    {\n",
    "                        \"description\" : {\"$eq\" : ''},\n",
    "                        \"content\" : {\"$eq\": None}\n",
    "                    }                    \n",
    "                ]\n",
    "            } \n",
    "        }, \n",
    "\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"id\" : 1\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if collection is None:\n",
    "        collection_list = db.list_collection_names()\n",
    "        for col in collection_list:\n",
    "            idsList = list(map(lambda x: x['_id'], db[col].aggregate(pipeline_remove)))\n",
    "            db[col].delete_many({'_id': {'$in': idsList}})\n",
    "            print(f\"{len(idsList)} documents with missing values were removed from {col}\\n\")\n",
    "    else:\n",
    "        idsList = list(map(lambda x: x['_id'], db[collection].aggregate(pipeline_remove)))\n",
    "        db[collection].delete_many({'_id': {'$in': idsList}})\n",
    "        print(f\"{len(idsList)} documents with missing values were removed from {collection}\\n\")\n",
    "\n",
    "remove_missing_values(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(db, collection=None): \n",
    "    \"\"\"\n",
    "    Function to remove documents with missing values on both description and content from db's specified collection\n",
    "    or all of them (default).\n",
    "    \"\"\"\n",
    "    pipeline_remove = [\n",
    "        {\n",
    "            \"$group\": {\n",
    "                \"_id\": {'description': '$description', 'content': '$content'},\n",
    "                \"_idsNeedsToBeDeleted\": {\"$push\": \"$$ROOT._id\"} # push all `_id`'s to an array\n",
    "            }\n",
    "        },\n",
    "        # Remove first element - which is removing a doc\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,\n",
    "                \"_idsNeedsToBeDeleted\": {  \n",
    "                    \"$slice\": [\n",
    "                        \"$_idsNeedsToBeDeleted\", 1, {\"$size\": \"$_idsNeedsToBeDeleted\"}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$unwind\": \"$_idsNeedsToBeDeleted\" # Unwind `_idsNeedsToBeDeleted`\n",
    "        },\n",
    "        # Group without a condition & push all `_idsNeedsToBeDeleted` fields to an array\n",
    "        {\n",
    "            \"$group\": { \"_id\": \"\", \"_idsNeedsToBeDeleted\": { \"$push\": \"$_idsNeedsToBeDeleted\" } }\n",
    "        },\n",
    "        { \n",
    "            \"$project\" : { \"_id\" : 0 }  # Optional stage\n",
    "        }\n",
    "        # At the end you'll have an [{ _idsNeedsToBeDeleted: [_ids] }] or []\n",
    "    ]\n",
    "    \n",
    "    if collection is None:\n",
    "        collection_list = db.list_collection_names()\n",
    "        for col in collection_list:\n",
    "            try:\n",
    "                idsList = list(db[col].aggregate(pipeline_remove))[0][\"_idsNeedsToBeDeleted\"]\n",
    "                db[col].delete_many({'_id': {'$in': idsList}})\n",
    "                print(f\"{len(idsList)} documents with duplicated documents were removed from {col}\\n\")\n",
    "            except IndexError:\n",
    "                print(f\"0 documents with duplicated documents in {col}\\n\")\n",
    "    else:\n",
    "        try:\n",
    "            idsList = list(db[collection].aggregate(pipeline_remove))[0][\"_idsNeedsToBeDeleted\"]\n",
    "            db[collection].delete_many({'_id': {'$in': idsList}})\n",
    "            print(f\"{len(idsList)} documents with duplicated documents were removed from {collection}\\n\")\n",
    "        except IndexError:\n",
    "                print(f\"0 documents with duplicated documents in {collection}\\n\")\n",
    "\n",
    "remove_duplicates(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### duplicates across collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {  # project fields\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'text': {\n",
    "                '$concat': [\n",
    "                    {'$ifNull': ['$title', '']},\n",
    "                    ' - ',\n",
    "                    {'$ifNull': ['$description', '']},\n",
    "                    ' - ',\n",
    "                    {'$ifNull': ['$content', '']},\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "r_everything = list(map(lambda x: x['text'], db.everything.aggregate(pipeline)))\n",
    "r_top_headlines = list(map(lambda x: x['text'], db.top_headlines.aggregate(pipeline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union allows to join the elements of two sets while removing the duplicates\n",
    "len(set(r_top_headlines).union(set(r_everything)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
