{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description:\n",
    "In this notebook we request 100 news articles from the NewsAPI (maximum allowed) and we use their truncated content to build a corpus, then we preprocess the corpus using the built CorpusPreprocess scikit-learn-like transformer. We train a gensim doc2vec model on the preprocessed corpus and we assess the model by checking document rankings for each document (the document should be the most similar with itself) and by comparing random documents' content with their similar documents' content. Finally, we request new documents from NewsAPI, apply preprocessing, infer their vectors and assess their quality by getting their most similar documents.\n",
    "\n",
    "# TODO:\n",
    "- add date, price, weekday, ... token to CorpusPreprocess\n",
    "- webscrape full content from urls provided by api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import CorpusPreprocess, check_random_doc_similarity, compare_documents, similarity_query\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import collections\n",
    "from newsapi import NewsApiClient\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import models\n",
    "# import numpy as np\n",
    "# from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "NEWSAPIKEY = os.environ.get(\"NEWSAPIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "newsapi = NewsApiClient(api_key=NEWSAPIKEY)\n",
    "\n",
    "# Get news articles\n",
    "articles = newsapi.get_top_headlines(language='en',\n",
    "                                     category='sports',  # 'business','entertainment','general','health','science','sports','technology'\n",
    "                                      # domains='bbc.co.uk',\n",
    "                                      # from_param=datetime.today() - timedelta(30),\n",
    "                                      # to=datetime.today(),\n",
    "                                      page_size=100,\n",
    "                                      country='us')\n",
    "\n",
    "corpus = list(set([c['content'] for c in articles['articles'] if c['content']]))\n",
    "\n",
    "print(\"Example of article content:\\n\\n{}\".format(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/ test split\n",
    "test_idx = random.sample(range(len(corpus)), int(len(corpus) * 0.1))\n",
    "test_corpus = [corpus[i] for i in test_idx]\n",
    "train_corpus = list(set(corpus).difference(set(test_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing - removing stopwords, lowercasing, strip accents, strip punctuation, stemming, max_df and min_df thresholds\n",
    "prep = CorpusPreprocess(stop_words=stopwords.words('english'), lowercase=True, strip_accents=True,\n",
    "                        strip_punctuation=punctuation, stemmer=True, max_df=0.2, min_df=2)\n",
    "processed_train_corpus = prep.fit_transform(train_corpus)\n",
    "processed_test_corpus = prep.transform(test_corpus)\n",
    "\n",
    "print(\"Example of preprocessed article content:\\n\\n{}\".format(processed_train_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaggedDocument format (input to doc2vec)\n",
    "tagged_corpus = [models.doc2vec.TaggedDocument(text, [i]) for i, text in enumerate(processed_train_corpus)]\n",
    "\n",
    "# Doc2Vec model\n",
    "model = models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=200)\n",
    "model.build_vocab(tagged_corpus)\n",
    "model.train(tagged_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab.keys()  # this accesses the words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing Doc2Vec model\n",
    "ranks = []\n",
    "for doc_id in range(len(tagged_corpus)):\n",
    "    inferred_vector = model.infer_vector(tagged_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "# Optimally we want as much documents to be the most similar with themselves (i.e. rank 0)\n",
    "print(collections.OrderedDict(sorted(collections.Counter(ranks).items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "Above we can the distribution of self-document similarity rank (i.e. ~ 53 documents have itself as the most similar document - rank 0, ~ 2 documents have itself as the second most similar document - rank 0, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the train corpus, infer its vector and check similarity with other documents\n",
    "doc_id, sims = check_random_doc_similarity(model, tagged_corpus)\n",
    "compare_documents(doc_id, train_corpus, sims, train_corpus)\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "# Pick a random document from the test corpus, infer its vector and check similarity with other documents\n",
    "doc_id, sims = check_random_doc_similarity(model, tagged_corpus, processed_test_corpus)\n",
    "compare_documents(doc_id, test_corpus, sims, train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new news articles\n",
    "new_articles = newsapi.get_everything(language='en',\n",
    "                                      domains='bbc.co.uk',\n",
    "                                      from_param=datetime.today() - timedelta(30),\n",
    "                                      to=datetime.today() - timedelta(20),\n",
    "                                      page_size=10)\n",
    "\n",
    "new_corpus = list(set([c['content'] for c in new_articles['articles'] if c['content']]))\n",
    "\n",
    "# Apply preprocessing\n",
    "new_processed_corpus = prep.transform(new_corpus)\n",
    "\n",
    "# Similarity query\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "unkwnown_doc = new_processed_corpus[doc_id]\n",
    "sims = similarity_query(model, unkwnown_doc)\n",
    "compare_documents(doc_id, new_corpus, sims, train_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
