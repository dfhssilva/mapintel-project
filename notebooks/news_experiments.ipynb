{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description:\n",
    "In this notebook we \n",
    "\n",
    "# TODO:\n",
    "- Use title instead of full content to build model\n",
    "- apply clustering,\n",
    "- use tf embedding projector or dash app to visualize embeddings\n",
    "- perform similarity queries based on input words/ sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import CorpusPreprocess, check_random_doc_similarity, compare_documents, export_test_results\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from gensim import models, corpora, matutils\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\", \"data\", \"raw\", \"bbc\")\n",
    "models_path = os.path.join(\"..\", \"models\", \"saved_models\")\n",
    "\n",
    "# Reading files into memory\n",
    "all_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(data_path)) for f in fn][1:]\n",
    "corpus = []\n",
    "for file in all_files:\n",
    "    with open(file, 'r') as f:\n",
    "        corpus.append(f.read())\n",
    "\n",
    "# Saving topics from each article\n",
    "topics = [path.split(\"\\\\\")[-2] for path in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/ test split\n",
    "train_corpus, test_corpus, train_topics, test_topics = train_test_split(corpus, topics, test_size=0.1, random_state=0)\n",
    "\n",
    "# Preprocessing\n",
    "prep = CorpusPreprocess(stop_words=stopwords.words('english'), lowercase=True, strip_accents=True,\n",
    "                        strip_punctuation=punctuation, stemmer=True, max_df=0.5, min_df=3)\n",
    "processed_train_corpus = prep.fit_transform(train_corpus)\n",
    "processed_test_corpus = prep.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BOW\n",
    "# dictionary = corpora.Dictionary(processed_train_corpus)\n",
    "# num_terms = len(dictionary.token2id)\n",
    "# num_train_docs = len(processed_train_corpus)\n",
    "# num_test_docs = len(processed_test_corpus)\n",
    "# vect_train_corpus = matutils.corpus2dense([dictionary.doc2bow(text) for text in processed_train_corpus],\n",
    "#                                           num_terms, num_train_docs)\n",
    "# vect_test_corpus = matutils.corpus2dense([dictionary.doc2bow(text) for text in processed_test_corpus],\n",
    "#                                           num_terms, num_test_docs)\n",
    "\n",
    "# # Train doc2vec model\n",
    "# TaggedDocument format (input to doc2vec)\n",
    "tagged_corpus = [models.doc2vec.TaggedDocument(text, [i]) for i, text in enumerate(processed_train_corpus)]\n",
    "# doc2vec_model = models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=200)\n",
    "# doc2vec_model.build_vocab(tagged_corpus)\n",
    "# # doc2vec_model.wv.vocab['later'].count  # this accesses the count of a word in the vocabulary\n",
    "# doc2vec_model.train(tagged_corpus, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "#\n",
    "# # Persist model to disk\n",
    "# doc2vec_model.save(os.path.join(models_path, \"doc2vec_model\"))\n",
    "\n",
    "# Load model from disk\n",
    "doc2vec_model = models.doc2vec.Doc2Vec.load(os.path.join(models_path, \"doc2vec_model.model\"))  # you can continue training with the loaded model!\n",
    "\n",
    "# # If you’re finished training a model (=no more updates, only querying, reduce memory usage), you can do\n",
    "# doc2vec_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing Doc2Vec model - takes some time to run\n",
    "ranks = []\n",
    "for doc_id in range(len(tagged_corpus)):\n",
    "    inferred_vector = doc2vec_model.infer_vector(tagged_corpus[doc_id].words)\n",
    "    sims = doc2vec_model.docvecs.most_similar([inferred_vector], topn=len(doc2vec_model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "# Optimally we want as much documents to be the most similar with themselves (i.e. rank 0)\n",
    "print(collections.OrderedDict(sorted(collections.Counter(ranks).items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "Above we can see the distribution of self-document similarity rank (i.e. ~ 1890 documents have itself as the most similar document - rank 0 and ~ 112 documents have itself as the second most similar document - rank 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the train corpus, infer its vector and check similarity with other documents\n",
    "doc_id, sims = check_random_doc_similarity(doc2vec_model, tagged_corpus)\n",
    "compare_documents(doc_id, train_corpus, sims, train_corpus)\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "# Pick a random document from the test corpus, infer its vector and check similarity with other documents\n",
    "doc_id, sims = check_random_doc_similarity(doc2vec_model, tagged_corpus, processed_test_corpus)\n",
    "compare_documents(doc_id, test_corpus, sims, train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query documents based on single words or small sentences\n",
    "query_input = 'Portugal economic crisis'\n",
    "prep_input = prep.transform([query_input])\n",
    "sims = doc2vec_model.docvecs.most_similar(positive=[doc2vec_model.infer_vector(prep_input[0])])\n",
    "print('Document ({}): «{}»\\n'.format(999, query_input))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS ACCORDING TO DOC2VEC:')\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims) // 2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], train_corpus[sims[index][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export test similarity queries of doc2vec\n",
    "# export_test_results(doc2vec_model, processed_test_corpus, test_corpus, train_corpus, \n",
    "#                     out_path=os.path.join(models_path, \"test_doc2vec.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing 2D T-SNE embedding space and document topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the vectorized corpus\n",
    "vect_train_corpus = np.vstack([doc2vec_model.docvecs[i] for i in range(len(train_corpus))])\n",
    "vect_test_corpus = np.vstack([doc2vec_model.infer_vector(i) for i in processed_test_corpus])\n",
    "# vect_mix_corpus = np.vstack([vect_train_corpus, vect_test_corpus])\n",
    "\n",
    "# Visualize a 2D map of the vectorized corpus\n",
    "tsne_model = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=5000,\n",
    "                  n_iter_without_progress=300, metric='cosine', verbose=1)\n",
    "\n",
    "embedded_train_corpus = tsne_model.fit_transform(vect_train_corpus)\n",
    "embedded_test_corpus = tsne_model.fit_transform(vect_test_corpus)\n",
    "# embedded_mix_corpus = np.vstack([embedded_train_corpus, embedded_test_corpus])\n",
    "\n",
    "categ_map = dict(zip(set(topics), ['red', 'blue', 'green', 'yellow', 'orange']))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "for ax, X, top, tit in zip(axes.flatten(), [embedded_train_corpus, embedded_test_corpus], [train_topics, test_topics],\n",
    "                           ['Train Corpus Embedding Space', 'Test Corpus Embedding Space']):\n",
    "    # Color for each point\n",
    "    color_points = list(map(lambda x: categ_map[x], top))\n",
    "    # Scatter plot\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=color_points)\n",
    "    # Produce a legend with the unique colors from the scatter\n",
    "    handles = [mpatches.Patch(color=c, label=l) for l, c in categ_map.items()]\n",
    "    ax.legend(handles=handles, loc=\"upper left\", title=\"Topics\", bbox_to_anchor=(0., 0.6, 0.4, 0.4))\n",
    "    # Set title\n",
    "    ax.set_title(tit)\n",
    "# plt.savefig(os.path.join(models_path, \"tsne_news_embeddings.png\"))  # exports png to current directory\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
